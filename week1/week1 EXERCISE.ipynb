{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os, requests, json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4401658-297b-439f-b5a3-08398a763be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "# question = \"\"\"\n",
    "# Please explain what this code does and why:\n",
    "# yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "# \"\"\"\n",
    "\n",
    "# question = \"\"\"\n",
    "# Please explain what would happen when you reach the context limit\n",
    "# of a LLM when using the API? Give me an example of what would happen\n",
    "# if i was to reach the content limit. Also explain the process of \n",
    "# how I would reach the limit.\n",
    "# \"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "When making using the API calls to a LLM for a conversation, do the order \n",
    "in which you are sending the user prompts matter?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1b65ace-70d5-4a1d-8eb0-db7b88c266ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a technical assistant to a mid level developer who is new to \\\n",
    "AI. Your job is to assist in answering any questions the user might have and give easy to \\\n",
    "understand answers. If applicable, come up with an example. Respond in Markdown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba9fb39-a104-4f30-bcc8-1741893e5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Please answer the following question:\\n\"\n",
    "user_prompt += question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, the order in which you send user prompts to a Language Model (LLM) does matter. \n",
       "\n",
       "### Why the Order Matters\n",
       "\n",
       "1. **Context**: LLMs like ChatGPT rely on the context of the conversation to generate relevant responses. If prompts are out of order, the model may not have the necessary context to understand what you're asking or referring to.\n",
       "\n",
       "2. **State Management**: The model maintains a state that represents the conversation history. If you skip back to an earlier point or jump ahead, it can lead to confusion in the response generated by the model.\n",
       "\n",
       "3. **Flow of Conversation**: Just like in a human conversation, there is a natural flow of dialogue. If you disrupt that flow, it could lead to misunderstandings or responses that don't fit well with the previous messages.\n",
       "\n",
       "### Example\n",
       "\n",
       "Consider a conversation flow:\n",
       "\n",
       "1. User: \"What is the capital of France?\"\n",
       "2. User: \"Can you tell me more about it?\"\n",
       "\n",
       "In this scenario, if you mixed the prompts like so:\n",
       "\n",
       "1. User: \"Can you tell me more about it?\"\n",
       "2. User: \"What is the capital of France?\"\n",
       "\n",
       "The model might not properly connect the second question to the appropriate context, leading to a less relevant or confusing response.\n",
       "\n",
       "### Best Practice\n",
       "\n",
       "Always send your user prompts in the order they were intended. If you need to refer back to previous information or a specific part of the conversation, try to summarize or quote it in your current prompt to maintain clarity. \n",
       "\n",
       "This will help ensure that the LLM has the context it needs to generate coherent and relevant responses!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model=MODEL_GPT,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "      ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Context Limits in LLMs\n",
       "========================\n",
       "\n",
       "No, not all Large Language Models (LLMs) inherently have strict context limits. However, some models may be designed to handle longer sequences of text or have specific features that address this challenge.\n",
       "\n",
       "**Models with context limits:**\n",
       "\n",
       "Some popular LLMs like BART, T5, and smaller variants of transformer-based models typically have a limited context window due to their architecture design. These models process input sequences in chunks (or windows) to maintain computational efficiency and memory usage. The size of the chunk depends on the specific model implementation.\n",
       "\n",
       "**Models without context limits:**\n",
       "\n",
       "However, there are LLMs designed to handle longer context or no context at all, such as:\n",
       "\n",
       "* **Transformer-XL**: This model is an extension of the Transformer architecture, which can process sequences up to several thousand tokens.\n",
       "* **Recurrent Neural Network (RNN) - based models**: Some RNN-based LLMs, like long short-term memory (LSTM), may not have a strict context limit. These models are typically trained on longer texts or sequences.\n",
       "\n",
       "**Why context limits?**\n",
       "\n",
       "Context limits can be beneficial for several reasons:\n",
       "\n",
       "* **Computational efficiency**: Handling larger contexts can increase computational requirements and lead to slower processing times.\n",
       "* **Memory usage**: Larger contexts require more memory, which may not be feasible in all environments.\n",
       "\n",
       "However, some applications (e.g., text summarization, language translation) often require processing longer sequences of text without context limits.\n",
       "\n",
       "**Conclusion**\n",
       "\n",
       "While many LLMs have built-in context limits, there are exceptions that can handle longer or no context at all. The choice of context limit depends on the specific model design and application requirements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "llama_messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "response = ollama.chat(model=MODEL_LLAMA, messages=llama_messages)\n",
    "# print(response['message']['content'])\n",
    "display(Markdown(response['message']['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b562a-76b0-45dd-ac20-c2be31f4c766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
